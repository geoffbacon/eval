{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "This goal of this notebook is to have a working implementation of standard models of learning distributed representations of words: word2vec and GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "There are a few existing implementations of word2vec. The [original code](https://code.google.com/archive/p/word2vec/) is available in C. The TensorFlow docs have a [good tutorial](https://www.tensorflow.org/tutorials/word2vec) with two versions of word2vec. However, I'm going with [gensim's](http://radimrehurek.com/gensim/models/word2vec.html) version. This is for the following reasons: 1) I'm confident it's correct, because it's listed on the website for the original version as a Python implementation, it's [fast](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), 3) it fits nicely into my existing Python workflow in a way that the other options don't, 4) it's used by other researchers. An easy tutorial showing how to use it is [here](https://rare-technologies.com/word2vec-tutorial/). To start, I'm going to train word2vec with default hyperparameters on an easy-to-use corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec in gensim comes with a bunch of different corpus objects to iterate over large corpora. It's straightforward to use your own corpus, but at first I'll use the pre-canned Brown corpus. The data of the Brown corpus come from NLTK but the pre-canned bit is gensim's class for iterating over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_brown = nltk.data.find('corpora/brown').path\n",
    "training_corpus = gensim.models.word2vec.BrownCorpus(path_to_brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Options for training word2vec in gensim\n",
    "- `sg` 0 for CBOW (default), 1 for skip-gram\n",
    "- `size` of vectors\n",
    "- `window` window size\n",
    "- `alpha` is the initial learning rate (will linearly drop to min_alpha as training progresses)\n",
    "- `seed` for setting random seed, but it's complicated in Python 3.\n",
    "- `min_count` lower bound on word frequency\n",
    "- `max_vocab_size` used to limit RAM usage\n",
    "- `workers` number of threads\n",
    "- `hs` 1 for hierarchical softmax, 0 for negative sampling (default)\n",
    "- `negative` number of negative words to sample (default 5)\n",
    "- `iter` number of epochs (default 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(training_corpus, sg=0, size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing trained word embeddings\n",
    "Gensim has a class, `keyedvectors` for storing word vectors in a read-only way. This is where gensim has all its functionality for assessing the vectors, such as accuracy on similarity/analogy datasets, most similar words, etc. Because it's a little restrictive, I don't know how much I'll use this. At the moment, I'd rather have them as a pandas dataframe and work with custom assessment methods from there. Most importantly, I want to save to a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37326825, -1.77543032,  1.01418495,  0.76122272,  0.6014545 ,\n",
       "        0.61599427, -0.40848544, -0.27209073,  1.14151788,  0.20488951,\n",
       "        0.67067975, -0.3072879 , -0.66000891, -0.51147223,  0.95049095,\n",
       "        0.86448979, -0.07529252,  0.12546216,  0.41390449,  0.79765427,\n",
       "       -0.51364458,  1.4868288 , -0.41729179,  1.49503493,  0.55480272,\n",
       "       -0.52688456,  0.26556841,  0.75362861, -0.09337724, -0.17674325,\n",
       "        1.00097585,  1.36793303,  1.11379051, -0.30979261, -0.01855088,\n",
       "       -0.03304428,  0.6429615 , -0.52750677, -1.76178443, -0.06411998,\n",
       "        0.24928731,  0.93973953, -0.19085188,  1.68263257, -0.24911675,\n",
       "       -0.44579032, -1.3018446 ,  0.08730339,  0.15827051,  1.31775475,\n",
       "       -0.38957056,  1.14586544,  0.29244775,  0.5834164 , -0.29266146,\n",
       "        0.20015037, -1.34383428,  1.21257615,  1.03930342, -0.99131125,\n",
       "       -0.89497703, -0.06676644,  1.34059489, -0.91830373,  0.23483844,\n",
       "        0.47910669, -0.39294741, -0.32439584,  0.5079456 ,  0.31104025,\n",
       "       -0.20484211,  1.31146431, -0.4195514 , -0.09104268, -0.19574967,\n",
       "       -1.4345082 ,  1.31452072,  0.85306364, -0.08649343, -0.98421258,\n",
       "        0.49436721,  1.08261847, -0.53990632, -0.32010564, -1.06853783,\n",
       "        0.42281196,  0.31815761,  0.09505003, -0.37981457, -0.53177595,\n",
       "       -0.96695548, -0.98959702,  1.18901324, -0.50033754,  0.31658953,\n",
       "        0.48240873, -0.2346075 ,  0.74937814,  0.02836621,  0.5906499 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the/at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model\n",
    "You can either save the whole model, which is good if you want to continue training it later, or just the word embeddings, which is best for my current purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save the whole model:\n",
    "#outfile = 'word2vec_model' # If model is large enough, gensim will actually write to multiple files\n",
    "#model.save(outfile, pickle_protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save just the embeddings\n",
    "outfile = 'word2vec_embeddings'\n",
    "embeddings.save_word2vec_format(outfile, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54294 100\r\n",
      "the/at 0.373268 -1.775430 1.014185 0.761223 0.601454 0.615994 -0.408485 -0.272091 1.141518 0.204890 0.670680 -0.307288 -0.660009 -0.511472 0.950491 0.864490 -0.075293 0.125462 0.413904 0.797654 -0.513645 1.486829 -0.417292 1.495035 0.554803 -0.526885 0.265568 0.753629 -0.093377 -0.176743 1.000976 1.367933 1.113791 -0.309793 -0.018551 -0.033044 0.642962 -0.527507 -1.761784 -0.064120 0.249287 0.939740 -0.190852 1.682633 -0.249117 -0.445790 -1.301845 0.087303 0.158271 1.317755 -0.389571 1.145865 0.292448 0.583416 -0.292661 0.200150 -1.343834 1.212576 1.039303 -0.991311 -0.894977 -0.066766 1.340595 -0.918304 0.234838 0.479107 -0.392947 -0.324396 0.507946 0.311040 -0.204842 1.311464 -0.419551 -0.091043 -0.195750 -1.434508 1.314521 0.853064 -0.086493 -0.984213 0.494367 1.082618 -0.539906 -0.320106 -1.068538 0.422812 0.318158 0.095050 -0.379815 -0.531776 -0.966955 -0.989597 1.189013 -0.500338 0.316590 0.482409 -0.234608 0.749378 0.028366 0.590650\r\n",
      "of/in -1.529803 -1.978377 0.310563 0.773821 0.270909 -0.733023 -1.352255 -0.291459 0.007918 0.077184 0.793518 1.391219 -0.451457 -1.499844 0.673062 0.658538 0.369698 -0.997085 -1.978150 1.710954 -0.728781 1.905465 0.148563 0.572467 0.149191 -1.289464 -1.753996 -0.164459 -0.424963 0.370950 -0.936709 0.419776 1.201306 0.726346 -0.020489 0.417041 -0.070219 -0.681623 -0.848683 0.616415 -0.177278 -0.095338 -1.438859 0.829488 -0.649056 -0.631096 -1.918885 -1.635585 1.666644 -0.414885 -0.904014 0.407269 0.250468 1.955814 0.337105 0.466630 -1.403512 0.742532 -0.655809 -0.551760 -0.037460 -0.249445 1.084324 -1.355897 0.244356 0.362318 -0.842265 -1.153346 0.868168 1.149714 0.734738 0.067628 -1.015216 -0.082311 -1.068647 -2.523809 1.501813 -0.927041 -0.412654 0.804185 -0.116760 -0.339383 -0.621143 0.664841 -0.008832 1.207955 -0.043193 0.109009 -1.811671 0.118273 -1.959882 -1.080844 1.682952 0.460006 -0.731722 -0.581894 -1.962956 -1.468441 0.152089 0.301374\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 word2vec_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of saved word embeddings is as follows: the first line is \"number of words in vocab size of embeddings\". Then, every other line is \"word form w1 w2 ... wn\". The format I want is as a pandas dataframe, with column labels being word forms, and n rows for the n dimensions. I want it this way because it's easier to access columns in pandas than rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('word2vec_embeddings', sep=' ', skiprows=[0], header=None, index_col=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the/at</th>\n",
       "      <th>of/in</th>\n",
       "      <th>and/cc</th>\n",
       "      <th>a/at</th>\n",
       "      <th>in/in</th>\n",
       "      <th>to/to</th>\n",
       "      <th>to/in</th>\n",
       "      <th>is/be</th>\n",
       "      <th>was/be</th>\n",
       "      <th>he/pp</th>\n",
       "      <th>...</th>\n",
       "      <th>fluke/nn</th>\n",
       "      <th>bilharziasis/nn</th>\n",
       "      <th>perelman/np</th>\n",
       "      <th>exhaling/vb</th>\n",
       "      <th>aviary/nn</th>\n",
       "      <th>olive-flushed/jj</th>\n",
       "      <th>cherokee/np</th>\n",
       "      <th>coral-colored/jj</th>\n",
       "      <th>boucle/nn</th>\n",
       "      <th>stupefying/vb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373268</td>\n",
       "      <td>-1.529803</td>\n",
       "      <td>-0.015312</td>\n",
       "      <td>0.544234</td>\n",
       "      <td>-0.738630</td>\n",
       "      <td>1.645025</td>\n",
       "      <td>-0.253535</td>\n",
       "      <td>-2.694053</td>\n",
       "      <td>-1.406520</td>\n",
       "      <td>0.779479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004336</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.007027</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.001828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.775430</td>\n",
       "      <td>-1.978377</td>\n",
       "      <td>-1.727973</td>\n",
       "      <td>-1.099333</td>\n",
       "      <td>-1.759403</td>\n",
       "      <td>-1.293590</td>\n",
       "      <td>-2.046214</td>\n",
       "      <td>-1.536931</td>\n",
       "      <td>-0.656673</td>\n",
       "      <td>0.824978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031166</td>\n",
       "      <td>-0.019416</td>\n",
       "      <td>-0.002642</td>\n",
       "      <td>-0.024017</td>\n",
       "      <td>-0.020490</td>\n",
       "      <td>-0.030188</td>\n",
       "      <td>-0.023442</td>\n",
       "      <td>-0.030332</td>\n",
       "      <td>-0.033190</td>\n",
       "      <td>-0.013628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.014185</td>\n",
       "      <td>0.310563</td>\n",
       "      <td>-0.302466</td>\n",
       "      <td>1.741750</td>\n",
       "      <td>-0.141368</td>\n",
       "      <td>-0.543366</td>\n",
       "      <td>-0.544071</td>\n",
       "      <td>1.306500</td>\n",
       "      <td>0.801294</td>\n",
       "      <td>-1.175240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010961</td>\n",
       "      <td>-0.007187</td>\n",
       "      <td>-0.002249</td>\n",
       "      <td>-0.009930</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.020248</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.026032</td>\n",
       "      <td>-0.022188</td>\n",
       "      <td>-0.006811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.761223</td>\n",
       "      <td>0.773821</td>\n",
       "      <td>-0.519353</td>\n",
       "      <td>1.287695</td>\n",
       "      <td>1.225874</td>\n",
       "      <td>-1.391787</td>\n",
       "      <td>0.163469</td>\n",
       "      <td>1.704131</td>\n",
       "      <td>1.788510</td>\n",
       "      <td>1.458925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.014442</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>-0.003881</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.003633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.601454</td>\n",
       "      <td>0.270909</td>\n",
       "      <td>0.459512</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>1.131693</td>\n",
       "      <td>-0.192218</td>\n",
       "      <td>0.880312</td>\n",
       "      <td>-0.955793</td>\n",
       "      <td>0.594803</td>\n",
       "      <td>0.884147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.003573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54294 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0    the/at     of/in    and/cc      a/at     in/in     to/to     to/in  \\\n",
       "1  0.373268 -1.529803 -0.015312  0.544234 -0.738630  1.645025 -0.253535   \n",
       "2 -1.775430 -1.978377 -1.727973 -1.099333 -1.759403 -1.293590 -2.046214   \n",
       "3  1.014185  0.310563 -0.302466  1.741750 -0.141368 -0.543366 -0.544071   \n",
       "4  0.761223  0.773821 -0.519353  1.287695  1.225874 -1.391787  0.163469   \n",
       "5  0.601454  0.270909  0.459512  0.923900  1.131693 -0.192218  0.880312   \n",
       "\n",
       "0     is/be    was/be     he/pp      ...        fluke/nn  bilharziasis/nn  \\\n",
       "1 -2.694053 -1.406520  0.779479      ...       -0.004336        -0.002706   \n",
       "2 -1.536931 -0.656673  0.824978      ...       -0.031166        -0.019416   \n",
       "3  1.306500  0.801294 -1.175240      ...       -0.010961        -0.007187   \n",
       "4  1.704131  1.788510  1.458925      ...        0.017467         0.014442   \n",
       "5 -0.955793  0.594803  0.884147      ...        0.013015         0.012107   \n",
       "\n",
       "0  perelman/np  exhaling/vb  aviary/nn  olive-flushed/jj  cherokee/np  \\\n",
       "1     0.001036     0.006756   0.008000         -0.001969    -0.007027   \n",
       "2    -0.002642    -0.024017  -0.020490         -0.030188    -0.023442   \n",
       "3    -0.002249    -0.009930  -0.001780         -0.020248    -0.021888   \n",
       "4     0.004862     0.002261   0.013614          0.006257    -0.003881   \n",
       "5     0.008290     0.009965   0.012712          0.008782     0.015151   \n",
       "\n",
       "0  coral-colored/jj  boucle/nn  stupefying/vb  \n",
       "1         -0.003263   0.002693       0.001828  \n",
       "2         -0.030332  -0.033190      -0.013628  \n",
       "3         -0.026032  -0.022188      -0.006811  \n",
       "4          0.006945   0.010526       0.003633  \n",
       "5          0.008668   0.011360       0.003573  \n",
       "\n",
       "[5 rows x 54294 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating embeddings\n",
    "As mentioned above, gensim's word2vec implementation has built-in functionality for assessing the embeddings. Although it won't always suit my purposes, I'm testing it here.\n",
    "\n",
    "OK, so there's a mismatch between the word form stored in the `keyedvectors` object and the way the words are stored in the 'wordsim353.tsv' file included with gensim. In particular, the training data has POS attached to it, whereas the wordsim dataset is just the word. I could find a workaround, but given how much other customization I want for evaluating embeddings, it's not worth it. Moreover, this sample test data included with gensim is in a particular format that the evaluation routines expect, and it's too restrictive for my purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#embeddings.evaluate_word_pairs(os.path.join(gensim.__path__[0], 'test', 'test_data', 'wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I evaluate against the ws-353 data myself. I want one dataframe with word1, word2, empirical_similarity and model_similarity. Then it should be easy to use the pandas `corr` method, `scipy.stats.spearmanr` or plot the data. The similarity data already has the first three columns, so I just add the model_similarity to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>which_set?</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>set1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2  similarity which_set?     1     2     3     4     5  \\\n",
       "0      love       sex        6.77       set1   9.0   6.0   8.0   8.0   7.0   \n",
       "1     tiger       cat        7.35       set1   9.0   7.0   8.0   7.0   8.0   \n",
       "2     tiger     tiger       10.00       set1  10.0  10.0  10.0  10.0  10.0   \n",
       "3      book     paper        7.46       set1   8.0   8.0   7.0   7.0   8.0   \n",
       "4  computer  keyboard        7.62       set1   8.0   7.0   9.0   9.0   8.0   \n",
       "\n",
       "      6     7     8     9    10    11    12    13  14  15  16  \n",
       "0   8.0   8.0   4.0   7.0   2.0   6.0   7.0   8.0 NaN NaN NaN  \n",
       "1   9.0   8.5   5.0   6.0   9.0   7.0   5.0   7.0 NaN NaN NaN  \n",
       "2  10.0  10.0  10.0  10.0  10.0  10.0  10.0  10.0 NaN NaN NaN  \n",
       "3   9.0   7.0   6.0   7.0   8.0   9.0   4.0   9.0 NaN NaN NaN  \n",
       "4   8.0   7.0   7.0   6.0   8.0  10.0   3.0   9.0 NaN NaN NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_ws353 = '../evaluate/data/ws-353/ws-353.csv'\n",
    "ws353 = pd.read_csv(path_to_ws353)\n",
    "ws353.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the Brown corpus I trained on has POS on the end of the words, so that 'love' isn't just 'love', but 'love/nn' and 'love/vb'. There are two different approaches I see to handling this:\n",
    "1. Always prefer one of the POSs (e.g. pretend 'love' always is 'love/nn').\n",
    "2. Calculate similarity for each POS, then average.\n",
    "\n",
    "The problem with 2 is that if word1 and word2 both have multiple POSs, then I have to calculate all possible pairwise similarities. For the time being, I'm going to go with 1. The first function `find_embedding` takes a word (without POS) and finds the column label for it in an embedding dataframe. Then `model_similarity` actually calculates the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_embedding(embeddings, w):\n",
    "    \"\"\"Helper function for finding vector representation of w in embeddings.\n",
    "    \n",
    "    Implements the logic of the disucssion above, namely that the Brown corpus\n",
    "    has POS tags while the similarity data doesn't. This is hacky.\n",
    "    \"\"\"\n",
    "    relevant_columns = [c for c in embeddings.columns if c.split('/')[0] == w]\n",
    "    assert len(relevant_columns) > 0, 'no embedding for {}'.format(w)\n",
    "    if len(relevant_columns) == 1:\n",
    "        column = relevant_columns[0]\n",
    "    elif len(relevant_columns) > 1:\n",
    "        pos = [c.split('/')[1] for c in relevant_columns]\n",
    "        if 'nn' in pos:\n",
    "            column = w + '/nn'\n",
    "        elif 'vb' in pos:\n",
    "            column = w + '/vb'\n",
    "        else:\n",
    "            column = relevant_columns[0]\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sugar/nn'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_embedding(df, 'sugar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine as cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_similarity(embeddings, word1, word2):\n",
    "    \"\"\"\n",
    "    Return the model's estimated similarity of word1 and word2.\n",
    "    \n",
    "    We can't use sklearn's pairwise cosine similarity because we \n",
    "    only want certain entries of that giant pairwise similarity matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : pandas.DataFrame\n",
    "        Of shape (num_dim, num_words)\n",
    "    word1, word2 : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Between 0 and 1\n",
    "    \"\"\"\n",
    "    word1, word2 = find_embedding(embeddings, word1), find_embedding(embeddings, word2)\n",
    "    v1, v2 = embeddings[word1], embeddings[word2]\n",
    "    return 1 - cosine_dist(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93076247876265705"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_similarity(df, 'love', 'sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If either of the words from the similarity dataset do not appear in the training data (or were dropped for frequency reasons), then they won't have an embedding and we can't calculate the model's similarity. In these cases, I'll leave their model_similarity as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(row):\n",
    "    \"\"\"\n",
    "    Helper function for applying model_similarity to the dataframe with empirical judgements.\n",
    "    \n",
    "    Note that the estimated embedding matrix `df` is baked in to this function, as the `apply` \n",
    "    function requires a one-argument function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        word1, word2 = row['word1'], row['word2']\n",
    "        return model_similarity(df, word1, word2)\n",
    "    except AssertionError: # either or both of the words are missing\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws353['estimate_word2vec'] = ws353.apply(evaluate_model, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>which_set?</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>estimate_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.930762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.797021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>set1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.858970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2  similarity which_set?     1     2     3     4     5  \\\n",
       "0      love       sex        6.77       set1   9.0   6.0   8.0   8.0   7.0   \n",
       "1     tiger       cat        7.35       set1   9.0   7.0   8.0   7.0   8.0   \n",
       "2     tiger     tiger       10.00       set1  10.0  10.0  10.0  10.0  10.0   \n",
       "3      book     paper        7.46       set1   8.0   8.0   7.0   7.0   8.0   \n",
       "4  computer  keyboard        7.62       set1   8.0   7.0   9.0   9.0   8.0   \n",
       "\n",
       "      6        ...             8     9    10    11    12    13  14  15  16  \\\n",
       "0   8.0        ...           4.0   7.0   2.0   6.0   7.0   8.0 NaN NaN NaN   \n",
       "1   9.0        ...           5.0   6.0   9.0   7.0   5.0   7.0 NaN NaN NaN   \n",
       "2  10.0        ...          10.0  10.0  10.0  10.0  10.0  10.0 NaN NaN NaN   \n",
       "3   9.0        ...           6.0   7.0   8.0   9.0   4.0   9.0 NaN NaN NaN   \n",
       "4   8.0        ...           7.0   6.0   8.0  10.0   3.0   9.0 NaN NaN NaN   \n",
       "\n",
       "   estimate_word2vec  \n",
       "0           0.930762  \n",
       "1           0.797021  \n",
       "2           1.000000  \n",
       "3           0.934245  \n",
       "4           0.858970  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now I have the ws353 data with an additional column for similarity estimated by word2vec (as trained above). We can use pandas's `corr` function, although that doesn't give us the p-value, like `scipy.stats.spearmanr` does. They both agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>estimate_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>similarity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimate_word2vec</th>\n",
       "      <td>-0.035086</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   similarity  estimate_word2vec\n",
       "similarity           1.000000          -0.035086\n",
       "estimate_word2vec   -0.035086           1.000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353[['similarity', 'estimate_word2vec']].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.035086466354430301, pvalue=0.53693587748827087)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353_without_nan = ws353[['similarity', 'estimate_word2vec']].dropna()\n",
    "spearmanr(ws353_without_nan['similarity'], ws353_without_nan['estimate_word2vec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO:\n",
    "- Logging\n",
    "- Training time\n",
    "- Train on bigger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
