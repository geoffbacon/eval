{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "This goal of this notebook is to have a working implementation of standard models of learning distributed representations of words: word2vec and GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "There are a few existing implementations of word2vec. The [original code](https://code.google.com/archive/p/word2vec/) is available in C. The TensorFlow docs have a [good tutorial](https://www.tensorflow.org/tutorials/word2vec) with two versions of word2vec. However, I'm going with [gensim's](http://radimrehurek.com/gensim/models/word2vec.html) version. This is for the following reasons: 1) I'm confident it's correct, because it's listed on the website for the original version as a Python implementation, it's [fast](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), 3) it fits nicely into my existing Python workflow in a way that the other options don't, 4) it's used by other researchers. An easy tutorial showing how to use it is [here](https://rare-technologies.com/word2vec-tutorial/). To start, I'm going to train word2vec with default hyperparameters on an easy-to-use corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec in gensim comes with a bunch of different corpus objects to iterate over large corpora. It's straightforward to use your own corpus, but at first I'll use the pre-canned Brown corpus. The data of the Brown corpus come from NLTK but the pre-canned bit is gensim's class for iterating over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_brown = nltk.data.find('corpora/brown').path\n",
    "training_corpus = gensim.models.word2vec.BrownCorpus(path_to_brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Options for training word2vec in gensim\n",
    "- `sg` 0 for CBOW (default), 1 for skip-gram\n",
    "- `size` of vectors\n",
    "- `window` window size\n",
    "- `alpha` is the initial learning rate (will linearly drop to min_alpha as training progresses)\n",
    "- `seed` for setting random seed, but it's complicated in Python 3.\n",
    "- `min_count` lower bound on word frequency\n",
    "- `max_vocab_size` used to limit RAM usage\n",
    "- `workers` number of threads\n",
    "- `hs` 1 for hierarchical softmax, 0 for negative sampling (default)\n",
    "- `negative` number of negative words to sample (default 5)\n",
    "- `iter` number of epochs (default 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(training_corpus, sg=0, size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing trained word embeddings\n",
    "Gensim has a class, `keyedvectors` for storing word vectors in a read-only way. This is where gensim has all its functionality for assessing the vectors, such as accuracy on similarity/analogy datasets, most similar words, etc. Because it's a little restrictive, I don't know how much I'll use this. At the moment, I'd rather have them as a pandas dataframe and work with custom assessment methods from there. Most importantly, I want to save to a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37326825, -1.77543032,  1.01418495,  0.76122272,  0.6014545 ,\n",
       "        0.61599427, -0.40848544, -0.27209073,  1.14151788,  0.20488951,\n",
       "        0.67067975, -0.3072879 , -0.66000891, -0.51147223,  0.95049095,\n",
       "        0.86448979, -0.07529252,  0.12546216,  0.41390449,  0.79765427,\n",
       "       -0.51364458,  1.4868288 , -0.41729179,  1.49503493,  0.55480272,\n",
       "       -0.52688456,  0.26556841,  0.75362861, -0.09337724, -0.17674325,\n",
       "        1.00097585,  1.36793303,  1.11379051, -0.30979261, -0.01855088,\n",
       "       -0.03304428,  0.6429615 , -0.52750677, -1.76178443, -0.06411998,\n",
       "        0.24928731,  0.93973953, -0.19085188,  1.68263257, -0.24911675,\n",
       "       -0.44579032, -1.3018446 ,  0.08730339,  0.15827051,  1.31775475,\n",
       "       -0.38957056,  1.14586544,  0.29244775,  0.5834164 , -0.29266146,\n",
       "        0.20015037, -1.34383428,  1.21257615,  1.03930342, -0.99131125,\n",
       "       -0.89497703, -0.06676644,  1.34059489, -0.91830373,  0.23483844,\n",
       "        0.47910669, -0.39294741, -0.32439584,  0.5079456 ,  0.31104025,\n",
       "       -0.20484211,  1.31146431, -0.4195514 , -0.09104268, -0.19574967,\n",
       "       -1.4345082 ,  1.31452072,  0.85306364, -0.08649343, -0.98421258,\n",
       "        0.49436721,  1.08261847, -0.53990632, -0.32010564, -1.06853783,\n",
       "        0.42281196,  0.31815761,  0.09505003, -0.37981457, -0.53177595,\n",
       "       -0.96695548, -0.98959702,  1.18901324, -0.50033754,  0.31658953,\n",
       "        0.48240873, -0.2346075 ,  0.74937814,  0.02836621,  0.5906499 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the/at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model\n",
    "You can either save the whole model, which is good if you want to continue training it later, or just the word embeddings, which is best for my current purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save the whole model:\n",
    "#outfile = 'word2vec_model' # If model is large enough, gensim will actually write to multiple files\n",
    "#model.save(outfile, pickle_protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save just the embeddings\n",
    "outfile = 'word2vec_embeddings'\n",
    "embeddings.save_word2vec_format(outfile, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54294 100\r\n",
      "the/at 0.373268 -1.775430 1.014185 0.761223 0.601454 0.615994 -0.408485 -0.272091 1.141518 0.204890 0.670680 -0.307288 -0.660009 -0.511472 0.950491 0.864490 -0.075293 0.125462 0.413904 0.797654 -0.513645 1.486829 -0.417292 1.495035 0.554803 -0.526885 0.265568 0.753629 -0.093377 -0.176743 1.000976 1.367933 1.113791 -0.309793 -0.018551 -0.033044 0.642962 -0.527507 -1.761784 -0.064120 0.249287 0.939740 -0.190852 1.682633 -0.249117 -0.445790 -1.301845 0.087303 0.158271 1.317755 -0.389571 1.145865 0.292448 0.583416 -0.292661 0.200150 -1.343834 1.212576 1.039303 -0.991311 -0.894977 -0.066766 1.340595 -0.918304 0.234838 0.479107 -0.392947 -0.324396 0.507946 0.311040 -0.204842 1.311464 -0.419551 -0.091043 -0.195750 -1.434508 1.314521 0.853064 -0.086493 -0.984213 0.494367 1.082618 -0.539906 -0.320106 -1.068538 0.422812 0.318158 0.095050 -0.379815 -0.531776 -0.966955 -0.989597 1.189013 -0.500338 0.316590 0.482409 -0.234608 0.749378 0.028366 0.590650\r\n",
      "of/in -1.529803 -1.978377 0.310563 0.773821 0.270909 -0.733023 -1.352255 -0.291459 0.007918 0.077184 0.793518 1.391219 -0.451457 -1.499844 0.673062 0.658538 0.369698 -0.997085 -1.978150 1.710954 -0.728781 1.905465 0.148563 0.572467 0.149191 -1.289464 -1.753996 -0.164459 -0.424963 0.370950 -0.936709 0.419776 1.201306 0.726346 -0.020489 0.417041 -0.070219 -0.681623 -0.848683 0.616415 -0.177278 -0.095338 -1.438859 0.829488 -0.649056 -0.631096 -1.918885 -1.635585 1.666644 -0.414885 -0.904014 0.407269 0.250468 1.955814 0.337105 0.466630 -1.403512 0.742532 -0.655809 -0.551760 -0.037460 -0.249445 1.084324 -1.355897 0.244356 0.362318 -0.842265 -1.153346 0.868168 1.149714 0.734738 0.067628 -1.015216 -0.082311 -1.068647 -2.523809 1.501813 -0.927041 -0.412654 0.804185 -0.116760 -0.339383 -0.621143 0.664841 -0.008832 1.207955 -0.043193 0.109009 -1.811671 0.118273 -1.959882 -1.080844 1.682952 0.460006 -0.731722 -0.581894 -1.962956 -1.468441 0.152089 0.301374\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 word2vec_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of saved word embeddings is as follows: the first line is \"number of words in vocab size of embeddings\". Then, every other line is \"word form w1 w2 ... wn\". The format I want is as a pandas dataframe, with column labels being word forms, and n rows for the n dimensions. I want it this way because it's easier to access columns in pandas than rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('word2vec_embeddings', sep=' ', skiprows=[0], header=None, index_col=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the/at</th>\n",
       "      <th>of/in</th>\n",
       "      <th>and/cc</th>\n",
       "      <th>a/at</th>\n",
       "      <th>in/in</th>\n",
       "      <th>to/to</th>\n",
       "      <th>to/in</th>\n",
       "      <th>is/be</th>\n",
       "      <th>was/be</th>\n",
       "      <th>he/pp</th>\n",
       "      <th>...</th>\n",
       "      <th>fluke/nn</th>\n",
       "      <th>bilharziasis/nn</th>\n",
       "      <th>perelman/np</th>\n",
       "      <th>exhaling/vb</th>\n",
       "      <th>aviary/nn</th>\n",
       "      <th>olive-flushed/jj</th>\n",
       "      <th>cherokee/np</th>\n",
       "      <th>coral-colored/jj</th>\n",
       "      <th>boucle/nn</th>\n",
       "      <th>stupefying/vb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373268</td>\n",
       "      <td>-1.529803</td>\n",
       "      <td>-0.015312</td>\n",
       "      <td>0.544234</td>\n",
       "      <td>-0.738630</td>\n",
       "      <td>1.645025</td>\n",
       "      <td>-0.253535</td>\n",
       "      <td>-2.694053</td>\n",
       "      <td>-1.406520</td>\n",
       "      <td>0.779479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004336</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.007027</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.001828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.775430</td>\n",
       "      <td>-1.978377</td>\n",
       "      <td>-1.727973</td>\n",
       "      <td>-1.099333</td>\n",
       "      <td>-1.759403</td>\n",
       "      <td>-1.293590</td>\n",
       "      <td>-2.046214</td>\n",
       "      <td>-1.536931</td>\n",
       "      <td>-0.656673</td>\n",
       "      <td>0.824978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031166</td>\n",
       "      <td>-0.019416</td>\n",
       "      <td>-0.002642</td>\n",
       "      <td>-0.024017</td>\n",
       "      <td>-0.020490</td>\n",
       "      <td>-0.030188</td>\n",
       "      <td>-0.023442</td>\n",
       "      <td>-0.030332</td>\n",
       "      <td>-0.033190</td>\n",
       "      <td>-0.013628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.014185</td>\n",
       "      <td>0.310563</td>\n",
       "      <td>-0.302466</td>\n",
       "      <td>1.741750</td>\n",
       "      <td>-0.141368</td>\n",
       "      <td>-0.543366</td>\n",
       "      <td>-0.544071</td>\n",
       "      <td>1.306500</td>\n",
       "      <td>0.801294</td>\n",
       "      <td>-1.175240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010961</td>\n",
       "      <td>-0.007187</td>\n",
       "      <td>-0.002249</td>\n",
       "      <td>-0.009930</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.020248</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.026032</td>\n",
       "      <td>-0.022188</td>\n",
       "      <td>-0.006811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.761223</td>\n",
       "      <td>0.773821</td>\n",
       "      <td>-0.519353</td>\n",
       "      <td>1.287695</td>\n",
       "      <td>1.225874</td>\n",
       "      <td>-1.391787</td>\n",
       "      <td>0.163469</td>\n",
       "      <td>1.704131</td>\n",
       "      <td>1.788510</td>\n",
       "      <td>1.458925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.014442</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>-0.003881</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.003633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.601454</td>\n",
       "      <td>0.270909</td>\n",
       "      <td>0.459512</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>1.131693</td>\n",
       "      <td>-0.192218</td>\n",
       "      <td>0.880312</td>\n",
       "      <td>-0.955793</td>\n",
       "      <td>0.594803</td>\n",
       "      <td>0.884147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.003573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54294 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0    the/at     of/in    and/cc      a/at     in/in     to/to     to/in  \\\n",
       "1  0.373268 -1.529803 -0.015312  0.544234 -0.738630  1.645025 -0.253535   \n",
       "2 -1.775430 -1.978377 -1.727973 -1.099333 -1.759403 -1.293590 -2.046214   \n",
       "3  1.014185  0.310563 -0.302466  1.741750 -0.141368 -0.543366 -0.544071   \n",
       "4  0.761223  0.773821 -0.519353  1.287695  1.225874 -1.391787  0.163469   \n",
       "5  0.601454  0.270909  0.459512  0.923900  1.131693 -0.192218  0.880312   \n",
       "\n",
       "0     is/be    was/be     he/pp      ...        fluke/nn  bilharziasis/nn  \\\n",
       "1 -2.694053 -1.406520  0.779479      ...       -0.004336        -0.002706   \n",
       "2 -1.536931 -0.656673  0.824978      ...       -0.031166        -0.019416   \n",
       "3  1.306500  0.801294 -1.175240      ...       -0.010961        -0.007187   \n",
       "4  1.704131  1.788510  1.458925      ...        0.017467         0.014442   \n",
       "5 -0.955793  0.594803  0.884147      ...        0.013015         0.012107   \n",
       "\n",
       "0  perelman/np  exhaling/vb  aviary/nn  olive-flushed/jj  cherokee/np  \\\n",
       "1     0.001036     0.006756   0.008000         -0.001969    -0.007027   \n",
       "2    -0.002642    -0.024017  -0.020490         -0.030188    -0.023442   \n",
       "3    -0.002249    -0.009930  -0.001780         -0.020248    -0.021888   \n",
       "4     0.004862     0.002261   0.013614          0.006257    -0.003881   \n",
       "5     0.008290     0.009965   0.012712          0.008782     0.015151   \n",
       "\n",
       "0  coral-colored/jj  boucle/nn  stupefying/vb  \n",
       "1         -0.003263   0.002693       0.001828  \n",
       "2         -0.030332  -0.033190      -0.013628  \n",
       "3         -0.026032  -0.022188      -0.006811  \n",
       "4          0.006945   0.010526       0.003633  \n",
       "5          0.008668   0.011360       0.003573  \n",
       "\n",
       "[5 rows x 54294 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating embeddings\n",
    "As mentioned above, gensim's word2vec implementation has built-in functionality for assessing the embeddings. Although it won't always suit my purposes, I'm testing it here.\n",
    "\n",
    "OK, so there's a mismatch between the word form stored in the `keyedvectors` object and the way the words are stored in the 'wordsim353.tsv' file included with gensim. In particular, the training data has POS attached to it, whereas the wordsim dataset is just the word. I could find a workaround, but given how much other customization I want for evaluating embeddings, it's not worth it. Moreover, this sample test data included with gensim is in a particular format that the evaluation routines expect, and it's too restrictive for my purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#embeddings.evaluate_word_pairs(os.path.join(gensim.__path__[0], 'test', 'test_data', 'wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I evaluate against the ws-353 data myself. I want one dataframe with word1, word2, empirical_similarity and model_similarity. Then it should be easy to use the pandas `corr` method, `scipy.stats.spearmanr` or plot the data. The similarity data already has the first three columns, so I just add the model_similarity to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>which_set?</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>set1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2  similarity which_set?     1     2     3     4     5  \\\n",
       "0      love       sex        6.77       set1   9.0   6.0   8.0   8.0   7.0   \n",
       "1     tiger       cat        7.35       set1   9.0   7.0   8.0   7.0   8.0   \n",
       "2     tiger     tiger       10.00       set1  10.0  10.0  10.0  10.0  10.0   \n",
       "3      book     paper        7.46       set1   8.0   8.0   7.0   7.0   8.0   \n",
       "4  computer  keyboard        7.62       set1   8.0   7.0   9.0   9.0   8.0   \n",
       "\n",
       "      6     7     8     9    10    11    12    13  14  15  16  \n",
       "0   8.0   8.0   4.0   7.0   2.0   6.0   7.0   8.0 NaN NaN NaN  \n",
       "1   9.0   8.5   5.0   6.0   9.0   7.0   5.0   7.0 NaN NaN NaN  \n",
       "2  10.0  10.0  10.0  10.0  10.0  10.0  10.0  10.0 NaN NaN NaN  \n",
       "3   9.0   7.0   6.0   7.0   8.0   9.0   4.0   9.0 NaN NaN NaN  \n",
       "4   8.0   7.0   7.0   6.0   8.0  10.0   3.0   9.0 NaN NaN NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_ws353 = '../evaluate/data/ws-353/ws-353.csv'\n",
    "ws353 = pd.read_csv(path_to_ws353)\n",
    "ws353.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the Brown corpus I trained on has POS on the end of the words, so that 'love' isn't just 'love', but 'love/nn' and 'love/vb'. There are two different approaches I see to handling this:\n",
    "1. Always prefer one of the POSs (e.g. pretend 'love' always is 'love/nn').\n",
    "2. Calculate similarity for each POS, then average.\n",
    "\n",
    "The problem with 2 is that if word1 and word2 both have multiple POSs, then I have to calculate all possible pairwise similarities. For the time being, I'm going to go with 1. The first function `find_embedding` takes a word (without POS) and finds the column label for it in an embedding dataframe. Then `model_similarity` actually calculates the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_embedding(embeddings, w):\n",
    "    \"\"\"Helper function for finding vector representation of w in embeddings.\n",
    "    \n",
    "    Implements the logic of the disucssion above, namely that the Brown corpus\n",
    "    has POS tags while the similarity data doesn't. This is hacky.\n",
    "    \"\"\"\n",
    "    relevant_columns = [c for c in embeddings.columns if c.split('/')[0] == w]\n",
    "    assert len(relevant_columns) > 0, 'no embedding for {}'.format(w)\n",
    "    if len(relevant_columns) == 1:\n",
    "        column = relevant_columns[0]\n",
    "    elif len(relevant_columns) > 1:\n",
    "        pos = [c.split('/')[1] for c in relevant_columns]\n",
    "        if 'nn' in pos:\n",
    "            column = w + '/nn'\n",
    "        elif 'vb' in pos:\n",
    "            column = w + '/vb'\n",
    "        else:\n",
    "            column = relevant_columns[0]\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sugar/nn'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_embedding(df, 'sugar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine as cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_similarity(embeddings, word1, word2):\n",
    "    \"\"\"\n",
    "    Return the model's estimated similarity of word1 and word2.\n",
    "    \n",
    "    We can't use sklearn's pairwise cosine similarity because we \n",
    "    only want certain entries of that giant pairwise similarity matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : pandas.DataFrame\n",
    "        Of shape (num_dim, num_words)\n",
    "    word1, word2 : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Between 0 and 1\n",
    "    \"\"\"\n",
    "    word1, word2 = find_embedding(embeddings, word1), find_embedding(embeddings, word2)\n",
    "    v1, v2 = embeddings[word1], embeddings[word2]\n",
    "    return 1 - cosine_dist(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93076247876265705"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_similarity(df, 'love', 'sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If either of the words from the similarity dataset do not appear in the training data (or were dropped for frequency reasons), then they won't have an embedding and we can't calculate the model's similarity. In these cases, I'll leave their model_similarity as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(row):\n",
    "    \"\"\"\n",
    "    Helper function for applying model_similarity to the dataframe with empirical judgements.\n",
    "    \n",
    "    Note that the estimated embedding matrix `df` is baked in to this function, as the `apply` \n",
    "    function requires a one-argument function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        word1, word2 = row['word1'], row['word2']\n",
    "        return model_similarity(df, word1, word2)\n",
    "    except AssertionError: # either or both of the words are missing\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws353['estimate_word2vec'] = ws353.apply(evaluate_model, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>which_set?</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>estimate_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.930762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.797021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>set1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.858970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2  similarity which_set?     1     2     3     4     5  \\\n",
       "0      love       sex        6.77       set1   9.0   6.0   8.0   8.0   7.0   \n",
       "1     tiger       cat        7.35       set1   9.0   7.0   8.0   7.0   8.0   \n",
       "2     tiger     tiger       10.00       set1  10.0  10.0  10.0  10.0  10.0   \n",
       "3      book     paper        7.46       set1   8.0   8.0   7.0   7.0   8.0   \n",
       "4  computer  keyboard        7.62       set1   8.0   7.0   9.0   9.0   8.0   \n",
       "\n",
       "      6        ...             8     9    10    11    12    13  14  15  16  \\\n",
       "0   8.0        ...           4.0   7.0   2.0   6.0   7.0   8.0 NaN NaN NaN   \n",
       "1   9.0        ...           5.0   6.0   9.0   7.0   5.0   7.0 NaN NaN NaN   \n",
       "2  10.0        ...          10.0  10.0  10.0  10.0  10.0  10.0 NaN NaN NaN   \n",
       "3   9.0        ...           6.0   7.0   8.0   9.0   4.0   9.0 NaN NaN NaN   \n",
       "4   8.0        ...           7.0   6.0   8.0  10.0   3.0   9.0 NaN NaN NaN   \n",
       "\n",
       "   estimate_word2vec  \n",
       "0           0.930762  \n",
       "1           0.797021  \n",
       "2           1.000000  \n",
       "3           0.934245  \n",
       "4           0.858970  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now I have the ws353 data with an additional column for similarity estimated by word2vec (as trained above). We can use pandas's `corr` function, although that doesn't give us the p-value, like `scipy.stats.spearmanr` does. They both agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>estimate_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>similarity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimate_word2vec</th>\n",
       "      <td>-0.035086</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   similarity  estimate_word2vec\n",
       "similarity           1.000000          -0.035086\n",
       "estimate_word2vec   -0.035086           1.000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353[['similarity', 'estimate_word2vec']].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.035086466354430301, pvalue=0.53693587748827087)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353_without_nan = ws353[['similarity', 'estimate_word2vec']].dropna()\n",
    "spearmanr(ws353_without_nan['similarity'], ws353_without_nan['estimate_word2vec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO:\n",
    "- Logging, esp training time\n",
    "- Train on bigger data\n",
    "- Wrap it up in a single function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "There are less existing implementations of GloVe to choose from. The [original/official implementation](https://nlp.stanford.edu/projects/glove/) is probably the best to use and I'll go with that. The next most promising implementation is [this one](https://github.com/GradySimon/tensorflow-glove) in TensorFlow. It's been tweeted by StanfordNLP so I imagine it's faithful. Then there are a handful of other Python implementations, but they haven't been widely used and don't look actively maintained, so I'm wary: [one](https://github.com/hans/glove.py), [two](https://github.com/JonathanRaiman/glove) and [three](https://github.com/maciejkula/glove-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'glove'...\n",
      "warning: redirecting to https://github.com/stanfordnlp/glove/\n",
      "remote: Counting objects: 361, done.\u001b[K\n",
      "remote: Total 361 (delta 0), reused 0 (delta 0), pack-reused 361\u001b[K\n",
      "Receiving objects: 100% (361/361), 145.37 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (194/194), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone http://github.com/stanfordnlp/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "gcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "\u001b[1msrc/glove.c:224:46: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mformat specifies type 'long' but the argument has\n",
      "      type 'long long' [-Wformat]\u001b[0m\n",
      "        if (write_header) fprintf(fout, \"%ld %d\\n\", vocab_size, vector_size);\n",
      "\u001b[0;1;32m                                         ~~~        ^~~~~~~~~~\n",
      "\u001b[0m\u001b[0;32m                                         %lld\n",
      "\u001b[0m1 warning generated.\n",
      "gcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "gcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "gcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n"
     ]
    }
   ],
   "source": [
    "!cd glove && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "gcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "\u001b[1msrc/glove.c:224:46: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mformat specifies type 'long' but the argument has\n",
      "      type 'long long' [-Wformat]\u001b[0m\n",
      "        if (write_header) fprintf(fout, \"%ld %d\\n\", vocab_size, vector_size);\n",
      "\u001b[0;1;32m                                         ~~~        ^~~~~~~~~~\n",
      "\u001b[0m\u001b[0;32m                                         %lld\n",
      "\u001b[0m1 warning generated.\n",
      "gcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "gcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "gcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "--2017-08-14 16:23:34--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net... 98.139.135.129\n",
      "Connecting to mattmahoney.net|98.139.135.129|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘text8.zip’\n",
      "\n",
      "text8.zip           100%[===================>]  29.89M  1.68MB/s    in 14s     \n",
      "\n",
      "2017-08-14 16:23:49 (2.08 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
      "\n",
      "Archive:  text8.zip\n",
      "  inflating: text8                   \n",
      "$ build/vocab_count -min-count 5 -verbose 2 < text8 > vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[11G1200000 tokens.\u001b[11G1300000 tokens.\u001b[11G1400000 tokens.\u001b[11G1500000 tokens.\u001b[11G1600000 tokens.\u001b[11G1700000 tokens.\u001b[11G1800000 tokens.\u001b[11G1900000 tokens.\u001b[11G2000000 tokens.\u001b[11G2100000 tokens.\u001b[11G2200000 tokens.\u001b[11G2300000 tokens.\u001b[11G2400000 tokens.\u001b[11G2500000 tokens.\u001b[11G2600000 tokens.\u001b[11G2700000 tokens.\u001b[11G2800000 tokens.\u001b[11G2900000 tokens.\u001b[11G3000000 tokens.\u001b[11G3100000 tokens.\u001b[11G3200000 tokens.\u001b[11G3300000 tokens.\u001b[11G3400000 tokens.\u001b[11G3500000 tokens.\u001b[11G3600000 tokens.\u001b[11G3700000 tokens.\u001b[11G3800000 tokens.\u001b[11G3900000 tokens.\u001b[11G4000000 tokens.\u001b[11G4100000 tokens.\u001b[11G4200000 tokens.\u001b[11G4300000 tokens.\u001b[11G4400000 tokens.\u001b[11G4500000 tokens.\u001b[11G4600000 tokens.\u001b[11G4700000 tokens.\u001b[11G4800000 tokens.\u001b[11G4900000 tokens.\u001b[11G5000000 tokens.\u001b[11G5100000 tokens.\u001b[11G5200000 tokens.\u001b[11G5300000 tokens.\u001b[11G5400000 tokens.\u001b[11G5500000 tokens.\u001b[11G5600000 tokens.\u001b[11G5700000 tokens.\u001b[11G5800000 tokens.\u001b[11G5900000 tokens.\u001b[11G6000000 tokens.\u001b[11G6100000 tokens.\u001b[11G6200000 tokens.\u001b[11G6300000 tokens.\u001b[11G6400000 tokens.\u001b[11G6500000 tokens.\u001b[11G6600000 tokens.\u001b[11G6700000 tokens.\u001b[11G6800000 tokens.\u001b[11G6900000 tokens.\u001b[11G7000000 tokens.\u001b[11G7100000 tokens.\u001b[11G7200000 tokens.\u001b[11G7300000 tokens.\u001b[11G7400000 tokens.\u001b[11G7500000 tokens.\u001b[11G7600000 tokens.\u001b[11G7700000 tokens.\u001b[11G7800000 tokens.\u001b[11G7900000 tokens.\u001b[11G8000000 tokens.\u001b[11G8100000 tokens.\u001b[11G8200000 tokens.\u001b[11G8300000 tokens.\u001b[11G8400000 tokens.\u001b[11G8500000 tokens.\u001b[11G8600000 tokens.\u001b[11G8700000 tokens.\u001b[11G8800000 tokens.\u001b[11G8900000 tokens.\u001b[11G9000000 tokens.\u001b[11G9100000 tokens.\u001b[11G9200000 tokens.\u001b[11G9300000 tokens.\u001b[11G9400000 tokens.\u001b[11G9500000 tokens.\u001b[11G9600000 tokens.\u001b[11G9700000 tokens.\u001b[11G9800000 tokens.\u001b[11G9900000 tokens.\u001b[11G10000000 tokens.\u001b[11G10100000 tokens.\u001b[11G10200000 tokens.\u001b[11G10300000 tokens.\u001b[11G10400000 tokens.\u001b[11G10500000 tokens.\u001b[11G10600000 tokens.\u001b[11G10700000 tokens.\u001b[11G10800000 tokens.\u001b[11G10900000 tokens.\u001b[11G11000000 tokens.\u001b[11G11100000 tokens.\u001b[11G11200000 tokens.\u001b[11G11300000 tokens.\u001b[11G11400000 tokens.\u001b[11G11500000 tokens.\u001b[11G11600000 tokens.\u001b[11G11700000 tokens.\u001b[11G11800000 tokens.\u001b[11G11900000 tokens.\u001b[11G12000000 tokens.\u001b[11G12100000 tokens.\u001b[11G12200000 tokens.\u001b[11G12300000 tokens.\u001b[11G12400000 tokens.\u001b[11G12500000 tokens.\u001b[11G12600000 tokens.\u001b[11G12700000 tokens.\u001b[11G12800000 tokens.\u001b[11G12900000 tokens.\u001b[11G13000000 tokens.\u001b[11G13100000 tokens.\u001b[11G13200000 tokens.\u001b[11G13300000 tokens.\u001b[11G13400000 tokens.\u001b[11G13500000 tokens.\u001b[11G13600000 tokens.\u001b[11G13700000 tokens.\u001b[11G13800000 tokens.\u001b[11G13900000 tokens.\u001b[11G14000000 tokens.\u001b[11G14100000 tokens.\u001b[11G14200000 tokens.\u001b[11G14300000 tokens.\u001b[11G14400000 tokens.\u001b[11G14500000 tokens.\u001b[11G14600000 tokens.\u001b[11G14700000 tokens.\u001b[11G14800000 tokens.\u001b[11G14900000 tokens.\u001b[11G15000000 tokens.\u001b[11G15100000 tokens.\u001b[11G15200000 tokens.\u001b[11G15300000 tokens.\u001b[11G15400000 tokens.\u001b[11G15500000 tokens.\u001b[11G15600000 tokens.\u001b[11G15700000 tokens.\u001b[11G15800000 tokens.\u001b[11G15900000 tokens.\u001b[11G16000000 tokens.\u001b[11G16100000 tokens.\u001b[11G16200000 tokens.\u001b[11G16300000 tokens.\u001b[11G16400000 tokens.\u001b[11G16500000 tokens.\u001b[11G16600000 tokens.\u001b[11G16700000 tokens.\u001b[11G16800000 tokens.\u001b[11G16900000 tokens.\u001b[11G17000000 tokens.\u001b[0GProcessed 17005207 tokens.\n",
      "Counted 253854 unique words.\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 71290.\n",
      "\n",
      "$ build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < text8 > cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"vocab.txt\"...loaded 71290 words.\n",
      "Building lookup table...table contains 94990279 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[19G6600000\u001b[19G6700000\u001b[19G6800000\u001b[19G6900000\u001b[19G7000000\u001b[19G7100000\u001b[19G7200000\u001b[19G7300000\u001b[19G7400000\u001b[19G7500000\u001b[19G7600000\u001b[19G7700000\u001b[19G7800000\u001b[19G7900000\u001b[19G8000000\u001b[19G8100000\u001b[19G8200000\u001b[19G8300000\u001b[19G8400000\u001b[19G8500000\u001b[19G8600000\u001b[19G8700000\u001b[19G8800000\u001b[19G8900000\u001b[19G9000000\u001b[19G9100000\u001b[19G9200000\u001b[19G9300000\u001b[19G9400000\u001b[19G9500000\u001b[19G9600000\u001b[19G9700000\u001b[19G9800000\u001b[19G9900000\u001b[19G10000000\u001b[19G10100000\u001b[19G10200000\u001b[19G10300000\u001b[19G10400000\u001b[19G10500000\u001b[19G10600000\u001b[19G10700000\u001b[19G10800000\u001b[19G10900000\u001b[19G11000000\u001b[19G11100000\u001b[19G11200000\u001b[19G11300000\u001b[19G11400000\u001b[19G11500000\u001b[19G11600000\u001b[19G11700000\u001b[19G11800000\u001b[19G11900000\u001b[19G12000000\u001b[19G12100000\u001b[19G12200000\u001b[19G12300000\u001b[19G12400000\u001b[19G12500000\u001b[19G12600000\u001b[19G12700000\u001b[19G12800000\u001b[19G12900000\u001b[19G13000000\u001b[19G13100000\u001b[19G13200000\u001b[19G13300000\u001b[19G13400000\u001b[19G13500000\u001b[19G13600000\u001b[19G13700000\u001b[19G13800000\u001b[19G13900000\u001b[19G14000000\u001b[19G14100000\u001b[19G14200000\u001b[19G14300000\u001b[19G14400000\u001b[19G14500000\u001b[19G14600000\u001b[19G14700000\u001b[19G14800000\u001b[19G14900000\u001b[19G15000000\u001b[19G15100000\u001b[19G15200000\u001b[19G15300000\u001b[19G15400000\u001b[19G15500000\u001b[19G15600000\u001b[19G15700000\u001b[19G15800000\u001b[19G15900000\u001b[19G16000000\u001b[19G16100000\u001b[19G16200000\u001b[19G16300000\u001b[19G16400000\u001b[19G16500000\u001b[19G16600000\u001b[19G16700000\u001b[19G16800000\u001b[19G16900000\u001b[19G17000000\u001b[0GProcessed 17005206 tokens.\n",
      "Writing cooccurrences to disk.........2 files in total.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[39G15700000 lines.\u001b[39G15800000 lines.\u001b[39G15900000 lines.\u001b[39G16000000 lines.\u001b[39G16100000 lines.\u001b[39G16200000 lines.\u001b[39G16300000 lines.\u001b[39G16400000 lines.\u001b[39G16500000 lines.\u001b[39G16600000 lines.\u001b[39G16700000 lines.\u001b[39G16800000 lines.\u001b[39G16900000 lines.\u001b[39G17000000 lines.\u001b[39G17100000 lines.\u001b[39G17200000 lines.\u001b[39G17300000 lines.\u001b[39G17400000 lines.\u001b[39G17500000 lines.\u001b[39G17600000 lines.\u001b[39G17700000 lines.\u001b[39G17800000 lines.\u001b[39G17900000 lines.\u001b[39G18000000 lines.\u001b[39G18100000 lines.\u001b[39G18200000 lines.\u001b[39G18300000 lines.\u001b[39G18400000 lines.\u001b[39G18500000 lines.\u001b[39G18600000 lines.\u001b[39G18700000 lines.\u001b[39G18800000 lines.\u001b[39G18900000 lines.\u001b[39G19000000 lines.\u001b[39G19100000 lines.\u001b[39G19200000 lines.\u001b[39G19300000 lines.\u001b[39G19400000 lines.\u001b[39G19500000 lines.\u001b[39G19600000 lines.\u001b[39G19700000 lines.\u001b[39G19800000 lines.\u001b[39G19900000 lines.\u001b[39G20000000 lines.\u001b[39G20100000 lines.\u001b[39G20200000 lines.\u001b[39G20300000 lines.\u001b[39G20400000 lines.\u001b[39G20500000 lines.\u001b[39G20600000 lines.\u001b[39G20700000 lines.\u001b[39G20800000 lines.\u001b[39G20900000 lines.\u001b[39G21000000 lines.\u001b[39G21100000 lines.\u001b[39G21200000 lines.\u001b[39G21300000 lines.\u001b[39G21400000 lines.\u001b[39G21500000 lines.\u001b[39G21600000 lines.\u001b[39G21700000 lines.\u001b[39G21800000 lines.\u001b[39G21900000 lines.\u001b[39G22000000 lines.\u001b[39G22100000 lines.\u001b[39G22200000 lines.\u001b[39G22300000 lines.\u001b[39G22400000 lines.\u001b[39G22500000 lines.\u001b[39G22600000 lines.\u001b[39G22700000 lines.\u001b[39G22800000 lines.\u001b[39G22900000 lines.\u001b[39G23000000 lines.\u001b[39G23100000 lines.\u001b[39G23200000 lines.\u001b[39G23300000 lines.\u001b[39G23400000 lines.\u001b[39G23500000 lines.\u001b[39G23600000 lines.\u001b[39G23700000 lines.\u001b[39G23800000 lines.\u001b[39G23900000 lines.\u001b[39G24000000 lines.\u001b[39G24100000 lines.\u001b[39G24200000 lines.\u001b[39G24300000 lines.\u001b[39G24400000 lines.\u001b[39G24500000 lines.\u001b[39G24600000 lines.\u001b[39G24700000 lines.\u001b[39G24800000 lines.\u001b[39G24900000 lines.\u001b[39G25000000 lines.\u001b[39G25100000 lines.\u001b[39G25200000 lines.\u001b[39G25300000 lines.\u001b[39G25400000 lines.\u001b[39G25500000 lines.\u001b[39G25600000 lines.\u001b[39G25700000 lines.\u001b[39G25800000 lines.\u001b[39G25900000 lines.\u001b[39G26000000 lines.\u001b[39G26100000 lines.\u001b[39G26200000 lines.\u001b[39G26300000 lines.\u001b[39G26400000 lines.\u001b[39G26500000 lines.\u001b[39G26600000 lines.\u001b[39G26700000 lines.\u001b[39G26800000 lines.\u001b[39G26900000 lines.\u001b[39G27000000 lines.\u001b[39G27100000 lines.\u001b[39G27200000 lines.\u001b[39G27300000 lines.\u001b[39G27400000 lines.\u001b[39G27500000 lines.\u001b[39G27600000 lines.\u001b[39G27700000 lines.\u001b[39G27800000 lines.\u001b[39G27900000 lines.\u001b[39G28000000 lines.\u001b[39G28100000 lines.\u001b[39G28200000 lines.\u001b[39G28300000 lines.\u001b[39G28400000 lines.\u001b[39G28500000 lines.\u001b[39G28600000 lines.\u001b[39G28700000 lines.\u001b[39G28800000 lines.\u001b[39G28900000 lines.\u001b[39G29000000 lines.\u001b[39G29100000 lines.\u001b[39G29200000 lines.\u001b[39G29300000 lines.\u001b[39G29400000 lines.\u001b[39G29500000 lines.\u001b[39G29600000 lines.\u001b[39G29700000 lines.\u001b[39G29800000 lines.\u001b[39G29900000 lines.\u001b[39G30000000 lines.\u001b[39G30100000 lines.\u001b[39G30200000 lines.\u001b[39G30300000 lines.\u001b[39G30400000 lines.\u001b[39G30500000 lines.\u001b[39G30600000 lines.\u001b[39G30700000 lines.\u001b[39G30800000 lines.\u001b[39G30900000 lines.\u001b[39G31000000 lines.\u001b[39G31100000 lines.\u001b[39G31200000 lines.\u001b[39G31300000 lines.\u001b[39G31400000 lines.\u001b[39G31500000 lines.\u001b[39G31600000 lines.\u001b[39G31700000 lines.\u001b[39G31800000 lines.\u001b[39G31900000 lines.\u001b[39G32000000 lines.\u001b[39G32100000 lines.\u001b[39G32200000 lines.\u001b[39G32300000 lines.\u001b[39G32400000 lines.\u001b[39G32500000 lines.\u001b[39G32600000 lines.\u001b[39G32700000 lines.\u001b[39G32800000 lines.\u001b[39G32900000 lines.\u001b[39G33000000 lines.\u001b[39G33100000 lines.\u001b[39G33200000 lines.\u001b[39G33300000 lines.\u001b[39G33400000 lines.\u001b[39G33500000 lines.\u001b[39G33600000 lines.\u001b[39G33700000 lines.\u001b[39G33800000 lines.\u001b[39G33900000 lines.\u001b[39G34000000 lines.\u001b[39G34100000 lines.\u001b[39G34200000 lines.\u001b[39G34300000 lines.\u001b[39G34400000 lines.\u001b[39G34500000 lines.\u001b[39G34600000 lines.\u001b[39G34700000 lines.\u001b[39G34800000 lines.\u001b[39G34900000 lines.\u001b[39G35000000 lines.\u001b[39G35100000 lines.\u001b[39G35200000 lines.\u001b[39G35300000 lines.\u001b[39G35400000 lines.\u001b[39G35500000 lines.\u001b[39G35600000 lines.\u001b[39G35700000 lines.\u001b[39G35800000 lines.\u001b[39G35900000 lines.\u001b[39G36000000 lines.\u001b[39G36100000 lines.\u001b[39G36200000 lines.\u001b[39G36300000 lines.\u001b[39G36400000 lines.\u001b[39G36500000 lines.\u001b[39G36600000 lines.\u001b[39G36700000 lines.\u001b[39G36800000 lines.\u001b[39G36900000 lines.\u001b[39G37000000 lines.\u001b[39G37100000 lines.\u001b[39G37200000 lines.\u001b[39G37300000 lines.\u001b[39G37400000 lines.\u001b[39G37500000 lines.\u001b[39G37600000 lines.\u001b[39G37700000 lines.\u001b[39G37800000 lines.\u001b[39G37900000 lines.\u001b[39G38000000 lines.\u001b[39G38100000 lines.\u001b[39G38200000 lines.\u001b[39G38300000 lines.\u001b[39G38400000 lines.\u001b[39G38500000 lines.\u001b[39G38600000 lines.\u001b[39G38700000 lines.\u001b[39G38800000 lines.\u001b[39G38900000 lines.\u001b[39G39000000 lines.\u001b[39G39100000 lines.\u001b[39G39200000 lines.\u001b[39G39300000 lines.\u001b[39G39400000 lines.\u001b[39G39500000 lines.\u001b[39G39600000 lines.\u001b[39G39700000 lines.\u001b[39G39800000 lines.\u001b[39G39900000 lines.\u001b[39G40000000 lines.\u001b[39G40100000 lines.\u001b[39G40200000 lines.\u001b[39G40300000 lines.\u001b[39G40400000 lines.\u001b[39G40500000 lines.\u001b[39G40600000 lines.\u001b[39G40700000 lines.\u001b[39G40800000 lines.\u001b[39G40900000 lines.\u001b[39G41000000 lines.\u001b[39G41100000 lines.\u001b[39G41200000 lines.\u001b[39G41300000 lines."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39G41400000 lines.\u001b[39G41500000 lines.\u001b[39G41600000 lines.\u001b[39G41700000 lines.\u001b[39G41800000 lines.\u001b[39G41900000 lines.\u001b[39G42000000 lines.\u001b[39G42100000 lines.\u001b[39G42200000 lines.\u001b[39G42300000 lines.\u001b[39G42400000 lines.\u001b[39G42500000 lines.\u001b[39G42600000 lines.\u001b[39G42700000 lines.\u001b[39G42800000 lines.\u001b[39G42900000 lines.\u001b[39G43000000 lines.\u001b[39G43100000 lines.\u001b[39G43200000 lines.\u001b[39G43300000 lines.\u001b[39G43400000 lines.\u001b[39G43500000 lines.\u001b[39G43600000 lines.\u001b[39G43700000 lines.\u001b[39G43800000 lines.\u001b[39G43900000 lines.\u001b[39G44000000 lines.\u001b[39G44100000 lines.\u001b[39G44200000 lines.\u001b[39G44300000 lines.\u001b[39G44400000 lines.\u001b[39G44500000 lines.\u001b[39G44600000 lines.\u001b[39G44700000 lines.\u001b[39G44800000 lines.\u001b[39G44900000 lines.\u001b[39G45000000 lines.\u001b[39G45100000 lines.\u001b[39G45200000 lines.\u001b[39G45300000 lines.\u001b[39G45400000 lines.\u001b[39G45500000 lines.\u001b[39G45600000 lines.\u001b[39G45700000 lines.\u001b[39G45800000 lines.\u001b[39G45900000 lines.\u001b[39G46000000 lines.\u001b[39G46100000 lines.\u001b[39G46200000 lines.\u001b[39G46300000 lines.\u001b[39G46400000 lines.\u001b[39G46500000 lines.\u001b[39G46600000 lines.\u001b[39G46700000 lines.\u001b[39G46800000 lines.\u001b[39G46900000 lines.\u001b[39G47000000 lines.\u001b[39G47100000 lines.\u001b[39G47200000 lines.\u001b[39G47300000 lines.\u001b[39G47400000 lines.\u001b[39G47500000 lines.\u001b[39G47600000 lines.\u001b[39G47700000 lines.\u001b[39G47800000 lines.\u001b[39G47900000 lines.\u001b[39G48000000 lines.\u001b[39G48100000 lines.\u001b[39G48200000 lines.\u001b[39G48300000 lines.\u001b[39G48400000 lines.\u001b[39G48500000 lines.\u001b[39G48600000 lines.\u001b[39G48700000 lines.\u001b[39G48800000 lines.\u001b[39G48900000 lines.\u001b[39G49000000 lines.\u001b[39G49100000 lines.\u001b[39G49200000 lines.\u001b[39G49300000 lines.\u001b[39G49400000 lines.\u001b[39G49500000 lines.\u001b[39G49600000 lines.\u001b[39G49700000 lines.\u001b[39G49800000 lines.\u001b[39G49900000 lines.\u001b[39G50000000 lines.\u001b[39G50100000 lines.\u001b[39G50200000 lines.\u001b[39G50300000 lines.\u001b[39G50400000 lines.\u001b[39G50500000 lines.\u001b[39G50600000 lines.\u001b[39G50700000 lines.\u001b[39G50800000 lines.\u001b[39G50900000 lines.\u001b[39G51000000 lines.\u001b[39G51100000 lines.\u001b[39G51200000 lines.\u001b[39G51300000 lines.\u001b[39G51400000 lines.\u001b[39G51500000 lines.\u001b[39G51600000 lines.\u001b[39G51700000 lines.\u001b[39G51800000 lines.\u001b[39G51900000 lines.\u001b[39G52000000 lines.\u001b[39G52100000 lines.\u001b[39G52200000 lines.\u001b[39G52300000 lines.\u001b[39G52400000 lines.\u001b[39G52500000 lines.\u001b[39G52600000 lines.\u001b[39G52700000 lines.\u001b[39G52800000 lines.\u001b[39G52900000 lines.\u001b[39G53000000 lines.\u001b[39G53100000 lines.\u001b[39G53200000 lines.\u001b[39G53300000 lines.\u001b[39G53400000 lines.\u001b[39G53500000 lines.\u001b[39G53600000 lines.\u001b[39G53700000 lines.\u001b[39G53800000 lines.\u001b[39G53900000 lines.\u001b[39G54000000 lines.\u001b[39G54100000 lines.\u001b[39G54200000 lines.\u001b[39G54300000 lines.\u001b[39G54400000 lines.\u001b[39G54500000 lines.\u001b[39G54600000 lines.\u001b[39G54700000 lines.\u001b[39G54800000 lines.\u001b[39G54900000 lines.\u001b[39G55000000 lines.\u001b[39G55100000 lines.\u001b[39G55200000 lines.\u001b[39G55300000 lines.\u001b[39G55400000 lines.\u001b[39G55500000 lines.\u001b[39G55600000 lines.\u001b[39G55700000 lines.\u001b[39G55800000 lines.\u001b[39G55900000 lines.\u001b[39G56000000 lines.\u001b[39G56100000 lines.\u001b[39G56200000 lines.\u001b[39G56300000 lines.\u001b[39G56400000 lines.\u001b[39G56500000 lines.\u001b[39G56600000 lines.\u001b[39G56700000 lines.\u001b[39G56800000 lines.\u001b[39G56900000 lines.\u001b[39G57000000 lines.\u001b[39G57100000 lines.\u001b[39G57200000 lines.\u001b[39G57300000 lines.\u001b[39G57400000 lines.\u001b[39G57500000 lines.\u001b[39G57600000 lines.\u001b[39G57700000 lines.\u001b[39G57800000 lines.\u001b[39G57900000 lines.\u001b[39G58000000 lines.\u001b[39G58100000 lines.\u001b[39G58200000 lines.\u001b[39G58300000 lines.\u001b[39G58400000 lines.\u001b[39G58500000 lines.\u001b[39G58600000 lines.\u001b[39G58700000 lines.\u001b[39G58800000 lines.\u001b[39G58900000 lines.\u001b[39G59000000 lines.\u001b[39G59100000 lines.\u001b[39G59200000 lines.\u001b[39G59300000 lines.\u001b[39G59400000 lines.\u001b[39G59500000 lines.\u001b[39G59600000 lines.\u001b[39G59700000 lines.\u001b[39G59800000 lines.\u001b[39G59900000 lines.\u001b[39G60000000 lines.\u001b[39G60100000 lines.\u001b[39G60200000 lines.\u001b[39G60300000 lines.\u001b[39G60400000 lines.\u001b[39G60500000 lines.\u001b[39G60600000 lines.\u001b[0GMerging cooccurrence files: processed 60666466 lines.\n",
      "\n",
      "$ build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 60666466 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G60666466 lines.\u001b[0GMerging temp files: processed 60666466 lines.\n",
      "\n",
      "$ build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 60666466 lines.\n",
      "Initializing parameters...done.\n",
      "vector size: 50\n",
      "vocab size: 71290\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "08/14/17 - 04:28.17PM, iter: 001, cost: 0.068961\n",
      "08/14/17 - 04:28.48PM, iter: 002, cost: 0.051682\n",
      "08/14/17 - 04:29.21PM, iter: 003, cost: 0.046141\n",
      "08/14/17 - 04:29.54PM, iter: 004, cost: 0.043013\n",
      "08/14/17 - 04:30.25PM, iter: 005, cost: 0.041169\n",
      "08/14/17 - 04:30.55PM, iter: 006, cost: 0.039964\n",
      "08/14/17 - 04:31.29PM, iter: 007, cost: 0.039105\n",
      "08/14/17 - 04:31.59PM, iter: 008, cost: 0.038458\n",
      "08/14/17 - 04:32.28PM, iter: 009, cost: 0.037922\n",
      "08/14/17 - 04:32.57PM, iter: 010, cost: 0.037509\n",
      "08/14/17 - 04:33.31PM, iter: 011, cost: 0.037163\n",
      "08/14/17 - 04:34.03PM, iter: 012, cost: 0.036855\n",
      "08/14/17 - 04:34.34PM, iter: 013, cost: 0.036618\n",
      "08/14/17 - 04:35.06PM, iter: 014, cost: 0.036388\n",
      "08/14/17 - 04:35.35PM, iter: 015, cost: 0.036183\n",
      "$ python eval/python/evaluate.py\n",
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 59.88% (303/506)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 26.96% (961/3564)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 3.69% (22/596)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 24.89% (580/2330)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 36.43% (153/420)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 5.04% (50/992)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 2.78% (21/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 25.30% (337/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 8.17% (81/992)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 14.11% (149/1056)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 56.94% (866/1521)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 14.17% (221/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 24.62% (328/1332)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 7.01% (61/870)\n",
      "Questions seen/total: 91.21% (17827/19544)\n",
      "Semantic accuracy: 27.22%  (2019/7416)\n",
      "Syntactic accuracy: 20.31%  (2114/10411)\n",
      "Total accuracy: 23.18%  (4133/17827)\n"
     ]
    }
   ],
   "source": [
    "!cd glove && ./demo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
