{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating Batchkarov et al. (2016) _A critique of word similarity as a method for evaluating distributional semantic models_\n",
    "\n",
    "This paper argues that the intrinsic evaluation of word embeddings with existing word similarity datasets is problematic. In particular, they argue the following:\n",
    "\n",
    "- that \"word similarity\" doesn't make sense outside of the context of a specific task,\n",
    "- that inter-annotator agreement is low on existing datasets,\n",
    "- and that the small size of existing datasets leads to too much variation in single number measures. \n",
    "\n",
    "I understand their conclusion to be that while word similarity can be used as a coarse evaluation, extrinsic methods should be preferred over intrinsic methods. They also have an interesting proposal for salvaging word similarity as an evaluation method, namely, use word similarity datasets that give increasingly worse evaluations to embeddings as random noise is added.\n",
    "\n",
    "While Batchkarov et al. do make [their code](https://github.com/mbatchkarov/repeval2016) public, I'd like to do it myself as a learning experience.\n",
    "\n",
    "They explore five word similarity datasets:\n",
    "\n",
    "|  Name  \t|              Paper             \t|\n",
    "|:------:\t|:------------------------------\t|\n",
    "|   RG   \t| Rubenstein & Goodenough (1965) \t|\n",
    "|   MC   \t|     Miller & Charles (1995)    \t|\n",
    "|  WS353 \t|    Finkelstein et al. (2001)   \t|\n",
    "|   MEN  \t|       Bruni et al. (2014)      \t|\n",
    "| SimLex \t|       Hill et al. (2015)       \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of word similarity\n",
    "\n",
    "Their first point is that \"word similarity\" could mean many different things. They note that many word similarity datasets do not distinguish/are not balanced for lexical semantic relationships (synonymy, antonymy, homonymy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Are the five datasets balanced balanced across lexical relationships?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Is there a relationship between lexical relationship and empirical similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Do the five datasets have POS distributions similar to natural text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Larger question: Are the five datasets representative of English? Are these words typical English words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They next note that the similarity judgements assigned to the same pair across datasets differs widely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../evaluate/data'\n",
    "\n",
    "rg65_path = 'rg-65/rg-65.csv'\n",
    "mc_path = 'mc/mc.csv'\n",
    "ws353_path = 'ws-353/ws-353.csv'\n",
    "men_path = 'men/men.csv'\n",
    "simlex_path = 'simlex/simlex.csv'\n",
    "\n",
    "rg65 = pd.read_csv(os.path.join(data_dir, rg65_path))\n",
    "mc = pd.read_csv(os.path.join(data_dir, mc_path))\n",
    "ws353 = pd.read_csv(os.path.join(data_dir, ws353_path))\n",
    "men = pd.read_csv(os.path.join(data_dir, men_path))\n",
    "simlex = pd.read_csv(os.path.join(data_dir, simlex_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>which_set?</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>set1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>set1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>set1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2  similarity which_set?     1     2     3     4     5  \\\n",
       "0      love       sex        6.77       set1   9.0   6.0   8.0   8.0   7.0   \n",
       "1     tiger       cat        7.35       set1   9.0   7.0   8.0   7.0   8.0   \n",
       "2     tiger     tiger       10.00       set1  10.0  10.0  10.0  10.0  10.0   \n",
       "3      book     paper        7.46       set1   8.0   8.0   7.0   7.0   8.0   \n",
       "4  computer  keyboard        7.62       set1   8.0   7.0   9.0   9.0   8.0   \n",
       "\n",
       "      6     7     8     9    10    11    12    13  14  15  16  \n",
       "0   8.0   8.0   4.0   7.0   2.0   6.0   7.0   8.0 NaN NaN NaN  \n",
       "1   9.0   8.5   5.0   6.0   9.0   7.0   5.0   7.0 NaN NaN NaN  \n",
       "2  10.0  10.0  10.0  10.0  10.0  10.0  10.0  10.0 NaN NaN NaN  \n",
       "3   9.0   7.0   6.0   7.0   8.0   9.0   4.0   9.0 NaN NaN NaN  \n",
       "4   8.0   7.0   7.0   6.0   8.0  10.0   3.0   9.0 NaN NaN NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, they point out that the pair \"chicken-rice\" has a normalized score of 0.14 in SimLex but 0.68 in MEN. And that the pair \"man-woman\" has 0.33 in SimLex but 0.84 in MEN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>similarity</th>\n",
       "      <th>word1_concreteness</th>\n",
       "      <th>word2_concreteness</th>\n",
       "      <th>concreteness_quartile</th>\n",
       "      <th>nelson_norms</th>\n",
       "      <th>top_333_in_nelson</th>\n",
       "      <th>similarity_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>chicken</td>\n",
       "      <td>rice</td>\n",
       "      <td>N</td>\n",
       "      <td>1.43</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word1 word2 POS  similarity  word1_concreteness  word2_concreteness  \\\n",
       "451  chicken  rice   N        1.43                 4.8                4.86   \n",
       "\n",
       "     concreteness_quartile  nelson_norms  top_333_in_nelson  similarity_sd  \n",
       "451                      4          0.27                  0           1.47  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex[(simlex['word1']=='chicken') & (simlex['word2']=='rice')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity_out_of_50</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>chicken</td>\n",
       "      <td>rice</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word1 word2  similarity_out_of_50  similarity\n",
       "909  chicken  rice                  34.0         6.8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men[(men['word1']=='chicken') & (men['word2']=='rice')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, I have two points here:\n",
    "- Is it OK to normalize similarity scores that weren't originally on a 1-10 scale? (What if the true measurement isn't linear?) I'll put this aside for now.\n",
    "- What other word pairs do these datasets have in common? Let's take a look.\n",
    "\n",
    "Merge datasets together on the two word columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = ['rg65', 'mc', 'ws353', 'men']\n",
    "df = simlex[['word1', 'word2', 'similarity']]\n",
    "df.columns = ['word1', 'word2', 'simlex']\n",
    "\n",
    "for name in datasets:\n",
    "    other = eval(name)[['word1', 'word2', 'similarity']]\n",
    "    other.columns = ['word1', 'word2', name]\n",
    "    df = pd.merge(df, other, how='outer', on=['word1', 'word2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word pairs present in more than one dataset will less than four NaN in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicates = df[['rg65', 'mc', 'ws353', 'men', 'simlex']].isnull().sum(axis=1) < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = df[duplicates][['simlex', 'rg65', 'mc', 'ws353', 'men']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is hacky, I'd like to have a better solution than this.\n",
    "values = df[duplicates][['simlex', 'rg65', 'mc', 'ws353', 'men']].values\n",
    "distances = []\n",
    "for row in values:\n",
    "    row = row.reshape(-1,1)\n",
    "    d = pdist(row)\n",
    "    distances.extend([i for i in d if not np.isnan(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the distribution of (absolute) discrepancies between the five datasets on a 10-point scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD3CAYAAADrGWTVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPYvRlWQ9iINidUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109e0df28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(distances, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.967480221776942e-08, 1.8331562835698065)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.halfnorm.fit(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distances are centered close enough to 0, with std 1.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, as Batchkarov-et-al point out, these datasets do give different estimates of word similarity from each other. The two examples they give are from the upper limit, as most absolute differences are less than 2. \n",
    "\n",
    "They go from this to the argument that a single global measure of word similarity is not appropriate, that each downstream task defines its own similarity measure. I can be sympathetic to that view, but I'm not sure of the link between inter-dataset variabaility and task-specific word similarity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity and task difficulty\n",
    "\n",
    "The previous section was largely about inter-dataset agreement. In this section, the authors look at inter-annotator agreement within two datasets: ws353 and MEN. These datasets come with individual level ratings, whereas MC and RG65 do not. SimLex comes with the standard deviation of the annotators' ratings. In SimLex, each pair was rated by approximately 50 humans. It seems a little odd that Batchkarov-et-al don't look at SimLex's inter-annotator agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK, now I'm a little confused by the mismatch between the text in the paper and the MEN dataset. The MEN dataset is 3,000 word pairs rated by *many* humans on MTurk. The resulting number per word pair is an aggregrate from all those judgements. Concretely, each of the 3,000 word pairs was randomly matched with another word pair 50 times. Each of those \"word-pair pairs\" was seen by *one* human, whose task was to say whether the first or the second word pair was more similar. Therefore, each word pair was ranked a total of 50 times against 50 different word pairs by 50 different humans. The score for a word pair is the number of times it was rated more similar than the other word pair. Then that score was normalized (by me). \n",
    "\n",
    "The density plot in the paper uses data from two annotators. Those two annotators are two of the authors of the paper. They rated each of the 3,000 word pairs on a 1-7 scale. Their data is not really part of the MEN dataset. I understand that data to be a simple sanity check of the methodology of the real MEN dataset. The authors calculate the correlation between the data they annotated and the data of the MTurkers. \n",
    "\n",
    "So in sum, I don't feel that the individual level data from the MEN dataset, collected from only two people, is a worthwhile dataset to talk about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also mention the variation in the ws353 dataset, saying that \"tiger-cat\" gets a normalized score from 0.5 to 0.9 from the 13 annotators. Let's look at how varied the ws353 is across all word pairs. And let's see where the \"tiger-cat\" data point is too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndqJNBD9MnboAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b1756a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings_cols = [str(i) for i in range(1, 17)]\n",
    "sns.distplot(ws353[ratings_cols].std(axis=1), kde=False);\n",
    "tiger_cat = ws353[(ws353['word1']=='tiger')&(ws353['word2']=='cat')][ratings_cols].std(axis=1).values[0]\n",
    "plt.axvline(tiger_cat, color='maroon');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so there is a large amount of variation across annotators within the ws353 dataset. It surprises me that Batchkarov-et-al used 'tiger-cat' as their example, when clearly there are more variable words to prove that point (although this plot would have been more informative). What's the most variable word pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word1    precedent\n",
       "word2      example\n",
       "Name: 134, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_variable = ws353[ratings_cols].std(axis=1).idxmax()\n",
    "ws353.loc[most_variable][['word1', 'word2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEFCAYAAABAVTQtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c15d208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(ws353.loc[most_variable][ratings_cols].astype(np.float64).dropna());\n",
    "sns.rugplot(ws353.loc[most_variable][ratings_cols].astype(np.float64).dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about as variable as you can get: everything from 1 to 10 for the pair \"precedent-example\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
